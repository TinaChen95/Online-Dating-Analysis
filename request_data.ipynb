{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 爬取 百合网(http://www.baihe.com/home.shtml) 的用户数据\n",
    "\n",
    "## 百合网站的模块\n",
    "### 1. 用户成功故事(http://story.baihe.com/ ) - 用户分享的恋爱故事列表\n",
    "<img style=\"float: middle;\"  src=\"故事列表.png\"  width=\"70%\">\n",
    "### 2. 用户个人空间(http://story.baihe.com/space.php?id=uid) \n",
    "###    每一位用户都有一个与个人id相关联的个人空间， 记载了该用户的个人恋爱故事\n",
    "<img style=\"float: middle;\"  src=\"个人空间.png\"  width=\"80%\">\n",
    "### 3. 用户基础数据(http://profile1.baihe.com/?oppID=uid) - 每一位用户都有一个与个人id个人展示页面\n",
    "<img style=\"float: middle;\"  src=\"个人信息.png\"  width=\"80%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import chardet\n",
    "from bs4 import BeautifulSoup\n",
    "import pickle\n",
    "\n",
    "def request_html(url):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'}\n",
    "    html = requests.get(url, headers=headers)\n",
    "    charset = chardet.detect(html.content)['encoding']\n",
    "    html = html.content.decode(charset)\n",
    "    return html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 爬取用户故事列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'state': '宣布结婚了', 'user': '87871348'}\n",
      "所有故事状态：\n",
      "{'', '宣布结婚了', '甜蜜恋爱了', '还在寻觅中', '开始约会了'}\n"
     ]
    }
   ],
   "source": [
    "base_url = 'http://story.baihe.com/story.php?story_list&p=%d'\n",
    "  \n",
    "story_list = dict()\n",
    "for i in range(1, 292):\n",
    "    html = request_html(base_url%i)\n",
    "    soup = BeautifulSoup(html,'html.parser')\n",
    "    for dd in soup.find('div',{'class':'storyMore'}).find_all('dd'):\n",
    "        try:\n",
    "            a = dd.div.find_all('a')\n",
    "            user_id = a[0].get('href').split('=')[-1]\n",
    "            story_id = a[1].get('href').split('=')[-1]\n",
    "        except:\n",
    "            print(i)\n",
    "            continue\n",
    "        state = dd.find('span',{'class':'state'}).text[5:]\n",
    "        story_list[story_id] = {'state': state, 'user': user_id}\n",
    "        \n",
    "print(story_list.pop('27274'))\n",
    "print('所有故事状态：')\n",
    "print({s['state'] for s in story_list.values()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "每个用户有多个故事出现在列表中\n"
     ]
    }
   ],
   "source": [
    "num_of_user = len({s['user'] for s in story_list.values()})\n",
    "num_of_story = len(story_list)\n",
    "if num_of_user == num_of_story:\n",
    "    print('每个用户只有一个故事出现在列表中')\n",
    "else:\n",
    "    print('每个用户有多个故事出现在列表中')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "816个用户\t宣布结婚了\n",
      "660个用户\t甜蜜恋爱了\n",
      " 78个用户\t开始约会了\n",
      " 39个用户\t甜蜜恋爱了->宣布结婚了\n",
      " 23个用户\t还在寻觅中\n",
      " 13个用户\t宣布结婚了->甜蜜恋爱了\n",
      "  6个用户\t开始约会了->甜蜜恋爱了\n",
      "  4个用户\t开始约会了->宣布结婚了\n",
      "  3个用户\t宣布结婚了->甜蜜恋爱了->宣布结婚了\n",
      "  2个用户\t还在寻觅中->甜蜜恋爱了\n",
      "  2个用户\t宣布结婚了->还在寻觅中\n",
      "  1个用户\t还在寻觅中->甜蜜恋爱了->宣布结婚了\n",
      "  1个用户\t甜蜜恋爱了->宣布结婚了->甜蜜恋爱了->宣布结婚了\n",
      "  1个用户\t甜蜜恋爱了->还在寻觅中\n",
      "  1个用户\t宣布结婚了->甜蜜恋爱了->宣布结婚了->甜蜜恋爱了->宣布结婚了->甜蜜恋爱了\n",
      "  1个用户\t甜蜜恋爱了->开始约会了\n"
     ]
    }
   ],
   "source": [
    "user2state = dict()\n",
    "story_id = sorted(story_list.keys(), key=lambda x: int(x))\n",
    "for story in story_id:\n",
    "    user = story_list[story]['user']\n",
    "    if not story_list[story]['state']:\n",
    "        continue\n",
    "    if user not in user2state:\n",
    "        user2state[user] = story_list[story]['state']\n",
    "    else:\n",
    "        if story_list[story]['state'] == user2state[user].split('->')[-1]:\n",
    "            continue\n",
    "        else:\n",
    "            user2state[user] += '->' + story_list[story]['state']\n",
    "\n",
    "from collections import Counter\n",
    "counter = Counter(user2state.values())\n",
    "items = sorted(counter.items(), key=lambda x:x[1], reverse=True)\n",
    "for key, value in items:\n",
    "    print('%3d个用户\\t%s'%(value, key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(story_list, open('story_list.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 爬取用户个人空间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 提取情侣id信息\n",
    "def user_infor(soup):\n",
    "    name = soup.p.text\n",
    "    if 'profile1' not in soup.a.get('href'):\n",
    "        return {}\n",
    "    uid = soup.a.get('href').split('=')[-1]\n",
    "    if uid == '-1':\n",
    "        return {}\n",
    "    try:\n",
    "        age = re.findall('(\\d+)',soup.a.text)[0]\n",
    "        area = soup.a.text.split()[-1]\n",
    "        img = soup.img.get('src')\n",
    "        return {'name':name, 'img':img, 'age':age, 'area':area, 'uid':uid}\n",
    "    except:\n",
    "        return {'uid':uid}\n",
    "\n",
    "# 提取个人空间页面信息\n",
    "def extract_kongjian(html):\n",
    "    data = dict()\n",
    "    soup = BeautifulSoup(html,'lxml')\n",
    "    \n",
    "    personal = soup.find('div',{'class':'personal'})\n",
    "    data['state'] = personal.find('div',{'class':'state'}).text\n",
    "    data['duration'] = re.sub('上传我的故事','',personal.find_all('li')[-1].text)\n",
    "    \n",
    "    user = personal.find_all('li')[:2]\n",
    "    data['user1'] = user_infor(user[0])\n",
    "    data['user2'] = user_infor(user[1])\n",
    "    \n",
    "    data['article'] = []\n",
    "    while True:\n",
    "        for div in soup.find_all('div',{'class':'article'}):\n",
    "            article = dict()\n",
    "            article['title'] = div.div.a.text\n",
    "            article['story_id'] = div.div.a.get('href').split('=')[-1]\n",
    "            article['time'] = div.div.span.text\n",
    "            article['content'] = re.sub('\\s','',div.find('div',{'class':'cont'}).text)\n",
    "            data['article'].append(article)\n",
    "        page_bar = soup.find('div',{'class':'page_now'})\n",
    "        if not page_bar:\n",
    "            break\n",
    "        else:\n",
    "            now = page_bar.find('span')\n",
    "            next_page = now.find_next('a')\n",
    "            if next_page.text == '后页':\n",
    "                break\n",
    "            else:\n",
    "                html = requests.get('http://story.baihe.com/'+next_page.get('href')).text\n",
    "                soup = BeautifulSoup(html,'lxml')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'state': '宣布结婚', 'duration': '211天后恋爱', 'user1': {'name': '云艾', 'img': 'http://photo12.baihe.com/2007/01/12/100_100/5FCEE5867FA1B6CCEA5E325648778799.jpg', 'age': '38', 'area': '福建福州', 'uid': '50122651'}, 'user2': {}, 'article': [{'title': '我们就要这样恋爱一辈子!', 'story_id': '547', 'time': '2007-08-03 15:05:17', 'content': '(1)2007年3月15日3/15消费者权益日，怎么会如此快就载入我的大脑CPU，那天太阳很大，气温有点高，我的脑袋有轻微的晕眩，犹豫了几分种，思索着该以什么样的形象、着装风格出现在你面前，突然觉得没有必要，就以一颗平常心吧，说不准发生什么呢，感觉是一种奇怪的东西。我宁愿相信自己的第6感，我想我们对彼此的印象应该都不错！周四过后，周日晚在温泉百合上岛咖啡厅，我们又见面...'}]}\n"
     ]
    }
   ],
   "source": [
    "stroy_list = pickle.load(open('story_list.pkl', 'rb'))\n",
    "users = {s['user'] for s in story_list.values()}\n",
    "users = [u for u in users if u != 'space.php']\n",
    "# kongjian = dict()\n",
    "base_url = 'http://story.baihe.com/space.php?id='\n",
    "for i, user in enumerate(users):\n",
    "    if user in ['53702191', '56648612']:\n",
    "        continue\n",
    "    if user in kongjian:\n",
    "        continue\n",
    "    html = requests.get(base_url + user).text\n",
    "    kongjian[user] = extract_kongjian(html)\n",
    "    if not kongjian[user]['user1'] or kongjian[user]['user1']['uid'] != user:\n",
    "        raise Exception\n",
    "        \n",
    "print(kongjian.pop('50122651'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 将所有人的伴侣加入待爬取列表\n",
    "users = {s['user'] for s in story_list.values()}\n",
    "users.remove('space.php')\n",
    "users.update({k['user2']['uid'] for k in kongjian.values() if k and 'uid' in k['user2']})\n",
    "for user in users:\n",
    "    if user in kongjian:\n",
    "        continue\n",
    "    html = requests.get(base_url + user).text\n",
    "    if len(html) == 20324:\n",
    "        kongjian[user] = {}\n",
    "        continue\n",
    "    else:\n",
    "        kongjian[user] = extract_kongjian(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(kongjian, open('kongjian2.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kongjian = pickle.load(open('kongjian2.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wrong = []\n",
    "for user, value in kongjian.items():\n",
    "    if len(value) == 0:\n",
    "        wrong.append(user)\n",
    "        continue\n",
    "    if 'uid' in value['user2'] and value['user2']['uid'] == user:\n",
    "        wrong.append(user)\n",
    "for w in wrong:\n",
    "    kongjian.pop(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58477845的情侣是71871256, 但是71871256的情侣是65190937\n",
      "50451794的情侣是1196009, 但是1196009的情侣是719161\n",
      "66271212的情侣是67311290, 但是67311290的情侣是66410644\n",
      "50701719的情侣是50642151, 但是50642151的情侣是3349658\n"
     ]
    }
   ],
   "source": [
    "lovers = dict()\n",
    "for user, value in kongjian.items():\n",
    "    if len(value) == 0:\n",
    "        continue\n",
    "    if len(value['user2']) == 0:\n",
    "        continue\n",
    "    lovers[user] = value['user2']['uid']\n",
    "            \n",
    "# 提取互为情侣的数据\n",
    "pairs = set()\n",
    "for user in lovers:\n",
    "    # 若A的伴侣是B，则B的伴侣应为A\n",
    "    if lovers[user] in lovers and lovers[lovers[user]] != user:\n",
    "        print('%s的情侣是%s, 但是%s的情侣是%s'%(user, lovers[user], lovers[user], lovers[lovers[user]]))\n",
    "    else:\n",
    "        pairs.add((min(user,lovers[user]),max(user,lovers[user])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 爬取所有的用户故事， 可以看出用户的恋爱状态是否发生变化了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_story(html):\n",
    "    data = dict()\n",
    "    soup = BeautifulSoup(html,'html.parser')\n",
    "    data['title'] = soup.h3.text\n",
    "    data['time'] = ' '.join(soup.h4.text.split()[:2])\n",
    "    data['article'] = soup.find('div',{'class':'details'}).div\n",
    "    personal = soup.find('div',{'class':'personal'})\n",
    "    data['state'] = personal.find('div',{'class':'state'}).text\n",
    "    data['duration'] = re.sub('上传我的故事','',personal.find_all('li')[-1].text)\n",
    "    \n",
    "    user = personal.find_all('li')[:2]\n",
    "    data['user1'] = user_infor(user[0])\n",
    "    data['user2'] = user_infor(user[1])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# stories = dict()\n",
    "for d in os.listdir('html2'):\n",
    "    if d in stories:\n",
    "        continue\n",
    "    with open('html2/'+d, 'r', encoding='utf8') as f:\n",
    "        html = f.read()\n",
    "    try:\n",
    "        stories[d] = extract_story(html)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12110"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'宣布结婚'}"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{v['state'] for v in stories.values()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_url = 'http://story.baihe.com/story.php?id='\n",
    "stories = dict()\n",
    "for i in range(1000, 27269):\n",
    "    html = base_url + str(i)\n",
    "    stories[str(i)] = extract_story(html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stories = {article['story_id'] for value in kongjian.values() for article in value['article']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27269\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "stories = {int(i) for i in stories}\n",
    "print(max(stories))\n",
    "print(min(stories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25001"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(27269 - 17) - len(stories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 提取用户信息 —— 需要登陆（通过设置cookies）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from extract_userinfo import *\n",
    "import json\n",
    "import re\n",
    "\n",
    "# 设置cookie\n",
    "from requests.cookies import RequestsCookieJar\n",
    "url = \"http://profile1.baihe.com/?showType=2012&oppID=78057814\"\n",
    " \n",
    "cookie_jar = RequestsCookieJar()\n",
    "with open('cookies.txt','r') as f:\n",
    "    for line in f.readlines():\n",
    "        key, value = line.split()[:2]\n",
    "        cookie_jar.set(key, value)\n",
    "\n",
    "authen_url = 'http://usericon.baihe.com/index/getUserAttestation?jsonCallBack=jQuery183016901437235994476_1532274807369&userID='\n",
    "def authen_info(uid):\n",
    "    text = requests.get(authen_url+uid).text\n",
    "    text = re.findall('\\((.*?)\\)',text)[0]\n",
    "    text = re.sub('null','0',text)\n",
    "    return json.loads(text)['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1400\n",
      "1600\n"
     ]
    }
   ],
   "source": [
    "blank = ['56168247', '76714707', '56332940', '1400', '56656421', '56570273']\n",
    "base_url = 'http://profile1.baihe.com/?oppID='\n",
    "users = {u for p in pairs for u in p}\n",
    "# woman = dict()\n",
    "# man = dict()\n",
    "for i, user in enumerate(users):\n",
    "    if user in man or user in woman:\n",
    "        continue\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "    if user in blank:\n",
    "        continue\n",
    "    data = authen_info(user)\n",
    "\n",
    "    url = base_url + user\n",
    "    html = requests.get(url, cookies=cookie_jar).text\n",
    "    if len(html) < 2500:\n",
    "        continue\n",
    "    soup = BeautifulSoup(html,'html.parser')\n",
    "    with open('users/%s' % user, 'w', encoding='utf8') as f:\n",
    "        f.write(str(soup.find('div',{'id':'BAIHE'})))\n",
    "\n",
    "    if data['genderChn'] == '她':\n",
    "        data.update(extract_woman(soup))\n",
    "        woman[data['userID']] = data\n",
    "    else:\n",
    "        data.update(extract_man(soup))\n",
    "        man[data['userID']] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(man, open('man.pkl', 'wb'))\n",
    "pickle.dump(woman, open('woman.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "woman = pickle.load(open('woman.pkl', 'rb'))\n",
    "man = pickle.load(open('man.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wrong = []\n",
    "for user1, user2 in pairs:\n",
    "    flag1 = user1 in man and user2 in woman\n",
    "    flag2 = user1 in woman and user2 in man\n",
    "    if flag1 or flag2:\n",
    "        continue\n",
    "    wrong.append((user1,user2))\n",
    "pairs = pairs - set(wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['userID', 'age', 'area', 'creditedNum', 'education', 'genderChn', 'height', 'intro', 'isCreditedByEducation', 'isCreditedById5', 'isCreditedByMobile', 'isCreditedBySesame', 'isCreditedBySfz', 'name', 'nickName', 'picture', 'status', 'tags', '住房', '体型', '体重', '偏爱约会方式', '公司性质', '公司行业', '厨艺状况', '婚姻', '婚姻状况', '学历', '宗教信仰', '家中排行', '家乡', '家务分工', '属相', '工作状况', '希望对方看重', '年龄', '恋爱类型', '想何时结婚', '户口', '所在地区', '所学专业', '择偶要求_有无子女', '掌握语言', '星座', '是否吸烟', '是否喝酒', '是否想要小孩', '月收入', '月薪', '有无子女', '期待婚礼形式', '母亲工作', '毕业学校', '民族', '父亲工作', '父母医保', '父母情况', '父母经济', '生活作息', '相貌自评', '职业', '血型', '购房', '购房情况', '购车', '身高']\n"
     ]
    }
   ],
   "source": [
    "keys = set()\n",
    "for v in woman.values():\n",
    "    keys = keys.union(v.keys())\n",
    "for v in man.values():\n",
    "    keys = keys.union(v.keys())\n",
    "keys.remove('userID')\n",
    "keys = ['userID'] + sorted(keys)\n",
    "print(keys)\n",
    "\n",
    "import csv\n",
    "def write_csv(file, data, keys):\n",
    "    with open(file, 'w', encoding='utf8', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(keys)\n",
    "        for value in data.values():\n",
    "            d = [value[key] if key in value else '' for key in keys]\n",
    "            writer.writerow(d)\n",
    "            \n",
    "# write_csv('man.csv', man, keys)\n",
    "write_csv('woman.csv', woman, keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'isCreditedById5', '身高', '购房情况', '有无子女', '属相', '恋爱类型', '月收入', '家乡', '是否吸烟', '住房', '星座', 'creditedNum', '父母医保', '婚姻', '体型', '希望对方看重', '父母情况', '厨艺状况', 'isCreditedBySesame', '公司行业', 'isCreditedBySfz', '相貌自评', 'education', '户口', '父亲工作', '婚姻状况', '父母经济', '宗教信仰', 'nickName', '期待婚礼形式', 'genderChn', '是否喝酒', '家务分工', '家中排行', '工作状况', '学历', '年龄', 'isCreditedByMobile', '是否想要小孩', 'intro', 'age', 'picture', '所在地区', '职业', 'isCreditedByEducation', '掌握语言', 'height', '择偶要求_有无子女', '偏爱约会方式', 'status', '月薪', '购车', '想何时结婚', '生活作息', '公司性质', 'userID', 'tags', '毕业学校', 'name', 'area', '民族', '所学专业', '血型', '母亲工作', '体重'}\n"
     ]
    }
   ],
   "source": [
    "mkeys = set()\n",
    "for v in man.values():\n",
    "    mkeys = mkeys.union(v.keys())\n",
    "print(mkeys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'想何时结婚', '生活作息', '公司性质', 'isCreditedById5', '公司行业', 'isCreditedBySfz', '身高', 'userID', '相貌自评', '购房情况', 'tags', '学历', '年龄', 'education', '有无子女', '户口', 'isCreditedByMobile', '是否想要小孩', 'intro', '父亲工作', '属相', '恋爱类型', '月收入', 'age', 'picture', '所在地区', '毕业学校', '家乡', '是否吸烟', '职业', '婚姻状况', 'isCreditedByEducation', '父母经济', '星座', '掌握语言', '宗教信仰', 'height', '择偶要求_有无子女', 'creditedNum', '父母医保', 'nickName', 'name', '期待婚礼形式', '体型', 'genderChn', '偏爱约会方式', '是否喝酒', 'area', '民族', '希望对方看重', '所学专业', 'status', '血型', '家务分工', '母亲工作', '月薪', '父母情况', '厨艺状况', '购车', '体重', '家中排行', '工作状况', 'isCreditedBySesame'}\n"
     ]
    }
   ],
   "source": [
    "print(keys.intersection(mkeys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'购房'}\n"
     ]
    }
   ],
   "source": [
    "print(keys-mkeys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'住房', '婚姻'}\n"
     ]
    }
   ],
   "source": [
    "print(mkeys-keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_man(soup):\n",
    "    data = dict()\n",
    "    data['name'] = soup.find('div',{'class':'name'}).text.split()[0]\n",
    "\n",
    "    a,b = soup.find('div',{'class':'manPic'}).dl.find_all('dt')[-1].text.split('：')\n",
    "    data[a] = b\n",
    "    \n",
    "    divs = soup.find_all('div',{'class':'inter'})\n",
    "    label = soup.find('div',{'class':'inter label'})\n",
    "    if label:\n",
    "        divs.pop(divs.index(label))\n",
    "        #data['sex'] = \n",
    "        data['tags'] = '/'.join([i.text for i in label.find_all('span')])\n",
    "    \n",
    "    #data['id'] = \n",
    "    splits = [i for i in re.findall('>(.*?)<', str(divs[-1])) if i != '/']\n",
    "    # splits = [i.strip() for i in soup.find('div',{'class':'inter'}).text.split('/')]\n",
    "    data['age'], data['height'], data['education'], data['area'] = splits[:4]\n",
    "    \n",
    "    # 是否有照片\n",
    "    div = soup.find('div',{'class':'manPic'}).find('div',{'class':'noPic'})\n",
    "    if div:\n",
    "        data['picture'] = '无'\n",
    "    else:\n",
    "        data['picture'] = '有'\n",
    "        \n",
    "    # 恋爱状态\n",
    "    div = soup.find('div',{'class':'womanPic'})\n",
    "    if not div:\n",
    "        div = soup.find('div',{'class':'manPic'})\n",
    "    if div.em:\n",
    "        data['status'] = div.em.text\n",
    "    else:\n",
    "        data['status'] = ''\n",
    "    \n",
    "    # 爬取表格\n",
    "    for infor in ['data', 'perData']:\n",
    "        div = soup.find('div',{'class': infor})\n",
    "        dt = div.find_all('dt')\n",
    "        dd = [d for d in div.find_all('dd') if not d.get('style')]\n",
    "        for i in range(len(dt)):\n",
    "            key = re.sub('\\s+','',dt[i].text[:-1])\n",
    "            value = re.sub('\\s+','',dd[i].text.strip())\n",
    "            if key in data and value != data[key]:\n",
    "                if key == '有无子女':\n",
    "                    data['择偶要求_有无子女'] = value\n",
    "                elif key == '相貌自评':\n",
    "                    continue\n",
    "                else:\n",
    "                    raise Exception('这个键有问题 %s %s %s' % (key, value, data[key]))\n",
    "            else:\n",
    "                data[key] = value\n",
    "    data['intro'] = re.sub('\\s+','',soup.find('div',{'class':'intr'}).text.strip())\n",
    "    if '择偶要求_有无子女' not in data:\n",
    "        data['择偶要求_有无子女'] = data['有无子女']\n",
    "    if '登录后可见' in data.values():\n",
    "        raise Exception('登陆问题')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_woman(soup):\n",
    "    data = dict()\n",
    "    data['name'] = soup.find('div',{'class':'name'}).text.split()[0]\n",
    "       \n",
    "    a,b = soup.find('div',{'class':'data'}).find_all('dt')[-1].text.split('：')\n",
    "    data[a] = b\n",
    "    \n",
    "    divs = soup.find_all('div',{'class':'inter'})\n",
    "    label = soup.find('div',{'class':'inter label'})\n",
    "    if label:\n",
    "        divs.pop(divs.index(label))\n",
    "        #data['sex'] = \n",
    "        data['tags'] = '/'.join([i.text for i in label.find_all('span')])\n",
    "    \n",
    "    #data['id'] = \n",
    "    splits = [i for i in re.findall('>(.*?)<', str(divs[-1])) if i != '/']\n",
    "    # splits = [i.strip() for i in soup.find('div',{'class':'inter'}).text.split('/')]\n",
    "    data['age'], data['height'], data['education'], data['area'] = splits[:4]\n",
    "    \n",
    "    # 是否有照片\n",
    "    div = soup.find('div',{'class':'womanPic'}).find('div',{'class':'noPic'})\n",
    "    if div:\n",
    "        data['picture'] = '无'\n",
    "    else:\n",
    "        data['picture'] = '有'\n",
    "        \n",
    "    # 恋爱状态\n",
    "    div = soup.find('div',{'class':'womanPic'})\n",
    "    if not div:\n",
    "        div = soup.find('div',{'class':'manPic'})\n",
    "    if div.em:\n",
    "        data['status'] = div.em.text\n",
    "    else:\n",
    "        data['status'] = ''\n",
    "    \n",
    "    # 爬取表格\n",
    "    infor = 'perData'\n",
    "    div = soup.find('div',{'class': infor})\n",
    "    dt = div.find_all('dt')\n",
    "    dd = [d for d in div.find_all('dd') if not d.get('style')]\n",
    "    for i in range(len(dt)):\n",
    "        key = re.sub('\\s+','',dt[i].text[:-1])\n",
    "        value = re.sub('\\s+','',dd[i].text.strip())\n",
    "        if key in data and value != data[key]:\n",
    "            if key == '有无子女':\n",
    "                data['择偶要求_有无子女'] = value\n",
    "            elif key == '相貌自评':\n",
    "                continue\n",
    "            else:\n",
    "                raise Exception('这个键有问题 %s %s %s' % (key, value, data[key]))\n",
    "        else:\n",
    "            data[key] = value\n",
    "    data['intro'] = re.sub('\\s+','',soup.find('div',{'class':'intr'}).text.strip())\n",
    "    \n",
    "    if '择偶要求_有无子女' not in data:\n",
    "        data['择偶要求_有无子女'] = data['有无子女']\n",
    "    if '登录后可见' in data.values():\n",
    "        raise Exception('登陆问题')\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
